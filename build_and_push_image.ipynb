{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c32776",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ceecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import os, shutil\n",
    "from distutils.dir_util import copy_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84663ba4",
   "metadata": {},
   "source": [
    "## Define source path for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9de82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define the path to the train and test data files on your local computer. \n",
    "\n",
    "src_path should be a directory which has two folders: 1) train, and 2) test\n",
    "'train' folder must have a file called: ratings_train.csv\n",
    "'test' folder must have a file called : ratings_test.csv \n",
    "\n",
    "Both 'ratings_train.csv' and 'ratings_text.csv' should have the following four fields (comma-separated): \n",
    "rating_id (string type)\n",
    "user_id (string type)\n",
    "item_id (string type)\n",
    "rating (float)\n",
    "'''\n",
    "\n",
    "\n",
    "dataset = 'jester'      # jester, movielens-10m, movielens-20m, book-crossing\n",
    "\n",
    "src_path = f'./data/{dataset}/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd132b62",
   "metadata": {},
   "source": [
    "## Make Volume to Mount with appropriate folders\n",
    "This is a folder created in this present directory. <br>\n",
    "We will copy the data files into this folder and then bind mount this folder into the container. <br>\n",
    "When the container is run, model artifacts and other outputs will also be saved to this same folder. <br>\n",
    "These files on the shared volume will persist when the container is shut down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab98404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this as-is. Do not change any folder names!!! \n",
    "\n",
    "mounted_volume = 'ml_vol'\n",
    "if os.path.exists(mounted_volume): shutil.rmtree(mounted_volume)\n",
    "\n",
    "os.mkdir(mounted_volume)\n",
    "subdirs = ['data', 'logs', 'model', 'output']\n",
    "for subdir in subdirs: \n",
    "    sub_dir_path = os.path.join(mounted_volume, subdir)\n",
    "    os.mkdir(sub_dir_path)\n",
    "\n",
    "subdirs = ['train', 'test']\n",
    "for subdir in subdirs: \n",
    "    sub_dir_path = os.path.join(mounted_volume, 'data', subdir)\n",
    "    os.mkdir(sub_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1a450",
   "metadata": {},
   "source": [
    "## Copy Data from Source Path Into Mounted Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7fcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = os.path.join(mounted_volume, 'data')\n",
    "for input_type in ['train', 'test']:\n",
    "    full_src = os.path.join(src_path, input_type)\n",
    "    full_dest = os.path.join(dest_path, input_type)\n",
    "    \n",
    "    if os.path.exists(full_src): copy_tree(full_src, full_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c644b2",
   "metadata": {},
   "source": [
    "# Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef4a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:0c9c7b267063808c0008323d31af185ea3c5d359441f8fc90d54b394570a03bb\n",
      "#1 transferring dockerfile: 38B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:e47969be58e26a961d0d1ceed55a8499806033a522914ac7edec300110e31e1d\n",
      "#2 transferring context: 34B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.8.0-slim\n",
      "#3 sha256:4f0c597550e30aa54f707f0017cf64d137017976c13b147baa6fd4ad0c55c91e\n",
      "#3 ...\n",
      "\n",
      "#4 [auth] library/python:pull token for registry-1.docker.io\n",
      "#4 sha256:7a1518bdb5466a1be4a0d129c88e7b814f729397fce85abba89bdfdf4642246f\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.8.0-slim\n",
      "#3 sha256:4f0c597550e30aa54f707f0017cf64d137017976c13b147baa6fd4ad0c55c91e\n",
      "#3 DONE 6.5s\n",
      "\n",
      "#5 [1/5] FROM docker.io/library/python:3.8.0-slim@sha256:8e243f41e500238f78f7a29a81656114d3fe603d5c34079a462d090f71c4b225\n",
      "#5 sha256:f2202870b184ece5b9a09c9b777f938cf0be25287ffe019e2c50e60191382ede\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 sha256:309946e9bba013b904527e44ad74e1cfa94735138354028bd391051141904fa7\n",
      "#6 transferring context: 32B\n",
      "#6 transferring context: 9.50kB 0.1s done\n",
      "#6 DONE 0.1s\n",
      "\n",
      "#7 [2/5] COPY ./requirements.txt .\n",
      "#7 sha256:c4003bb17e0ef2870f697a081ce0bec00b5583e6c03541d48722f74eee5ac956\n",
      "#7 CACHED\n",
      "\n",
      "#8 [3/5] RUN pip3 install -r requirements.txt\n",
      "#8 sha256:b36e84c721180b00bdfa367a8256abc6768a92ad6cf18afd77a29d54ae76cbd4\n",
      "#8 CACHED\n",
      "\n",
      "#9 [4/5] COPY . .\n",
      "#9 sha256:ba52d5036f0687e436cd05da3702136c3fae08731d467bbb426df5d2abe0e2d3\n",
      "#9 DONE 0.0s\n",
      "\n",
      "#10 [5/5] WORKDIR /app\n",
      "#10 sha256:5c2bddcc013bdc4e401b9d2ddc239cdafb9573286c753571ccfef64ad5a55186\n",
      "#10 DONE 0.0s\n",
      "\n",
      "#11 exporting to image\n",
      "#11 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#11 exporting layers 0.1s done\n",
      "#11 writing image sha256:e587516b408ce9e2d0b49b60606e38effe0138d49a995fb27a09f2ab159b1e84 done\n",
      "#11 naming to docker.io/abudesai/rec_base_mf:1 done\n",
      "#11 DONE 0.1s\n",
      "\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t abudesai/rec_base_mf:1 ./mf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da862320",
   "metadata": {},
   "source": [
    "# Create Container From Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcb7243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01d7d039df9ab8aa97bfa52d9479e342e062ba62332d802e11c570a6541e564d\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "declare vol_path=\"$(pwd)/ml_vol\"\n",
    "docker run -d -p 3000:3000 -v $vol_path:/app/ml_vol --name mfc abudesai/rec_base_mf:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d850",
   "metadata": {},
   "source": [
    "# Check Container and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc34742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                    COMMAND                  CREATED          STATUS          PORTS                    NAMES\n",
      "01d7d039df9a   abudesai/rec_base_mf:1   \"/bin/sh -c 'python â€¦\"   19 seconds ago   Up 17 seconds   0.0.0.0:3000->3000/tcp   mfc\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e9fb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash \n",
    "#docker inspect mfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63677bbb",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a833472a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2021-12-06 23:35:28.620173: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-06 23:35:28.620221: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-06 23:35:30.260466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-06 23:35:30.260507: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-06 23:35:30.260519: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3f983bdb18de): /proc/driver/nvidia/version does not exist\n",
      "2021-12-06 23:35:30.260628: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker exec mfc python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a3e96",
   "metadata": {},
   "source": [
    "# Run Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28747aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape:  (57489, 3)\n",
      "proc_test_data shape:  (44729, 6)\n",
      "preds shape:  (44729, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:52:25.106476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-06 23:52:25.106516: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-06 23:52:28.533509: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-06 23:52:28.533551: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-06 23:52:28.533562: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3f983bdb18de): /proc/driver/nvidia/version does not exist\n",
      "2021-12-06 23:52:28.533680: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker exec mfc python predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817579e9",
   "metadata": {},
   "source": [
    "# Score Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e40846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score {'mse': 11.984519340055028, 'rmse': 3.4618664532380548, 'mae': 2.7503276394158536, 'nmae': 1.0240949426300248, 'smape': 156.6801513808124, 'r2': 0.17162261001815127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 23:52:59.839827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-06 23:52:59.839867: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker exec mfc python score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae22ee9",
   "metadata": {},
   "source": [
    "# Check Outputs In mounted volume\n",
    "- data: this is where we mounted training and test data\n",
    "- logs: contains logged model training output\n",
    "- model: trained model artifacts\n",
    "- output: contains predictions. Also contains output from hyper-parameter tuning, if run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3bda16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml_vol:\n",
      "data\n",
      "output\n",
      "score\n",
      "\n",
      "ml_vol/data:\n",
      "test\n",
      "train\n",
      "\n",
      "ml_vol/data/test:\n",
      "ratings_test.csv\n",
      "\n",
      "ml_vol/data/train:\n",
      "attribute_defn.csv\n",
      "ratings_train.csv\n",
      "user_attributes.csv\n",
      "\n",
      "ml_vol/output:\n",
      "scores.csv\n",
      "\n",
      "ml_vol/score:\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -R ml_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c715c",
   "metadata": {},
   "source": [
    "# Push to docker hub \n",
    "Need permission to push to this repository on docker hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28d65813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/abudesai/rec_base_mf]\n",
      "5f70bf18a086: Preparing\n",
      "2bcb74ebc7e3: Preparing\n",
      "34541a4460b5: Preparing\n",
      "6463cb47ff88: Preparing\n",
      "d82a4c4e92ff: Preparing\n",
      "155411760e3a: Preparing\n",
      "2cbb114c7605: Preparing\n",
      "459d9d53a256: Preparing\n",
      "831c5620387f: Preparing\n",
      "155411760e3a: Waiting\n",
      "459d9d53a256: Waiting\n",
      "831c5620387f: Waiting\n",
      "2cbb114c7605: Waiting\n",
      "5f70bf18a086: Layer already exists\n",
      "6463cb47ff88: Layer already exists\n",
      "d82a4c4e92ff: Layer already exists\n",
      "459d9d53a256: Layer already exists\n",
      "2cbb114c7605: Layer already exists\n",
      "155411760e3a: Layer already exists\n",
      "831c5620387f: Layer already exists\n",
      "2bcb74ebc7e3: Pushed\n",
      "34541a4460b5: Pushed\n",
      "1: digest: sha256:8da6cd2a74f5b5542bb1f50ac19abfc3ba880ab9ef03d53d0e63968dcf4b35da size: 2208\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push abudesai/rec_base_mf:1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521774e1",
   "metadata": {},
   "source": [
    "# Stop Container and Remove Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker stop mfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b39202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker rm mfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker rmi abudesai/rec_base_mf:1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
