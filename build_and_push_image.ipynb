{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00c32776",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ceecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import os, shutil\n",
    "from distutils.dir_util import copy_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84663ba4",
   "metadata": {},
   "source": [
    "## Define source path for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9de82b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define the path to the train and test data files on your local computer. \n",
    "\n",
    "src_path should be a directory which has two folders: 1) train, and 2) test\n",
    "'train' folder must have a file called: ratings_train.csv\n",
    "'test' folder must have a file called : ratings_test.csv \n",
    "\n",
    "Both 'ratings_train.csv' and 'ratings_test.csv' should have the following four fields (comma-separated): \n",
    "rating_id (string type)\n",
    "user_id (string type)\n",
    "item_id (string type)\n",
    "rating (float)\n",
    "\n",
    "You may use the test file purely for predictions, i.e. without doing scoring. In that case, the file may exclude the 'rating'\n",
    "column. \n",
    "'''\n",
    "\n",
    "\n",
    "dataset = 'jester'      # jester, movielens-10m, movielens-20m, book-crossing\n",
    "\n",
    "src_path = f'./data/{dataset}/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd132b62",
   "metadata": {},
   "source": [
    "## Make Volume to Mount with appropriate folders\n",
    "This is a folder created in this present directory. <br>\n",
    "We will copy the data files into this folder and then bind mount this folder into the container. <br>\n",
    "When the container is run, model artifacts and other outputs will also be saved to this same folder. <br>\n",
    "These files on the shared volume will persist when the container is shut down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ab98404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this as-is. Do not change any folder names!!! \n",
    "\n",
    "mounted_volume = 'ml_vol'\n",
    "if os.path.exists(mounted_volume): shutil.rmtree(mounted_volume)\n",
    "\n",
    "os.mkdir(mounted_volume)\n",
    "subdirs = ['data', 'logs', 'model', 'output']\n",
    "for subdir in subdirs: \n",
    "    sub_dir_path = os.path.join(mounted_volume, subdir)\n",
    "    os.mkdir(sub_dir_path)\n",
    "\n",
    "subdirs = ['train', 'test']\n",
    "for subdir in subdirs: \n",
    "    sub_dir_path = os.path.join(mounted_volume, 'data', subdir)\n",
    "    os.mkdir(sub_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1a450",
   "metadata": {},
   "source": [
    "## Copy Data from Source Path Into Mounted Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c7fcd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_path = os.path.join(mounted_volume, 'data')\n",
    "for input_type in ['train', 'test']:\n",
    "    full_src = os.path.join(src_path, input_type)\n",
    "    full_dest = os.path.join(dest_path, input_type)\n",
    "    \n",
    "    if os.path.exists(full_src): copy_tree(full_src, full_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c644b2",
   "metadata": {},
   "source": [
    "# Build Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ef4a40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:0e64079d6a4de6de8a4dd5cb3f748919bdba76c975b309b289e3dc9519977512\n",
      "#1 transferring dockerfile: 38B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:543414db0c54ed2c0ff1716c9b1e6f3615c1ee23a93199f391289d68d9b3d3cd\n",
      "#2 transferring context: 34B 0.0s done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.8.0-slim\n",
      "#3 sha256:4f0c597550e30aa54f707f0017cf64d137017976c13b147baa6fd4ad0c55c91e\n",
      "#3 ...\n",
      "\n",
      "#4 [auth] library/python:pull token for registry-1.docker.io\n",
      "#4 sha256:5050d93228b82dd142f137a4710be50b6aa1a99f5f3264a22e478a6aafc8740d\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.8.0-slim\n",
      "#3 sha256:4f0c597550e30aa54f707f0017cf64d137017976c13b147baa6fd4ad0c55c91e\n",
      "#3 DONE 12.0s\n",
      "\n",
      "#10 [1/5] FROM docker.io/library/python:3.8.0-slim@sha256:8e243f41e500238f78f7a29a81656114d3fe603d5c34079a462d090f71c4b225\n",
      "#10 sha256:f2202870b184ece5b9a09c9b777f938cf0be25287ffe019e2c50e60191382ede\n",
      "#10 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 sha256:a45a7c3711130147475615ac96380e2f712269e12ec9eabca3c29436ecf15c1b\n",
      "#5 transferring context: 1.62kB 0.1s done\n",
      "#5 DONE 0.1s\n",
      "\n",
      "#6 [2/5] COPY ./requirements.txt .\n",
      "#6 sha256:b970cf37d4680f02ff7766f3b45e46ac57280671b056da50971aa1dfe5505584\n",
      "#6 CACHED\n",
      "\n",
      "#7 [3/5] RUN pip3 install -r requirements.txt\n",
      "#7 sha256:f9f0db774ad30a52a9636da7a42fa39029c0c0a87df9d43ccf681ee597dbc820\n",
      "#7 CACHED\n",
      "\n",
      "#8 [4/5] COPY . .\n",
      "#8 sha256:addd8fb6278e5014c78c58ef50efb4597bfc70d8b2277aa5713e1eaba27fc8db\n",
      "#8 CACHED\n",
      "\n",
      "#9 [5/5] WORKDIR /app\n",
      "#9 sha256:a0c0337fe3a6d9389e94064a2c3db9bcd11508536aa3063c8177514873a75b6a\n",
      "#9 CACHED\n",
      "\n",
      "#11 exporting to image\n",
      "#11 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#11 exporting layers done\n",
      "#11 writing image sha256:9a804ecc151617e347ea43d016e81c7045db3655be2836e010253380f9e84e0d done\n",
      "#11 naming to docker.io/abudesai/rec_base_mf:latest done\n",
      "#11 DONE 0.0s\n",
      "\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker build -t abudesai/rec_base_mf:latest ./mf_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da862320",
   "metadata": {},
   "source": [
    "# Create Container From Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcb7243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c433fbff4a2aa09e9f02b9da7151cc2eaed2f2cfd3210339326a34cd56d92d1a\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "declare vol_path=\"$(pwd)/ml_vol\"\n",
    "docker run -d -p 3000:3000 -v $vol_path:/app/ml_vol --name mf_gd abudesai/rec_base_mf:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d850",
   "metadata": {},
   "source": [
    "# Check Container and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7dc34742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                         COMMAND                  CREATED             STATUS             PORTS                    NAMES\n",
      "c433fbff4a2a   abudesai/rec_base_mf:latest   \"/bin/sh -c 'python â€¦\"   About an hour ago   Up About an hour   0.0.0.0:3000->3000/tcp   mf_gd\n",
      "31582b63b4a2   myapp                         \"python first-pythonâ€¦\"   2 hours ago         Up 2 hours         0.0.0.0:8080->8080/tcp   myappc\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e9fb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash \n",
    "#docker inspect mfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63677bbb",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a833472a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2021-12-14 23:37:33.260042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-14 23:37:33.260081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-14 23:37:35.138594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-14 23:37:35.138636: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-14 23:37:35.138651: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c433fbff4a2a): /proc/driver/nvidia/version does not exist\n",
      "2021-12-14 23:37:35.138779: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "docker exec mf_gd python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a3e96",
   "metadata": {},
   "source": [
    "# Run Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d85da7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prediction data... \n",
      "test data shape:  (206818, 4)\n",
      "Loading trained Matrix_Factorizer... \n",
      "Making predictions... \n",
      "proc_test_data shape:  (206818, 7)\n",
      "preds shape:  (206818, 1)\n",
      "Saving predictions... \n",
      "Done with predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 00:28:22.120251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 00:28:22.120348: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-15 00:28:24.157762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-15 00:28:24.157885: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-15 00:28:24.157917: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c433fbff4a2a): /proc/driver/nvidia/version does not exist\n",
      "2021-12-15 00:28:24.158293: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "!docker exec mf_gd python predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "28747aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prediction data... \n",
      "test data shape:  (206818, 4)\n",
      "Loading trained Matrix_Factorizer... \n",
      "Making predictions... \n",
      "proc_test_data shape:  (206818, 7)\n",
      "preds shape:  (206818, 1)\n",
      "Saving predictions... \n",
      "Done with predictions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 00:28:47.754337: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 00:28:47.754437: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-15 00:28:49.514068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-15 00:28:49.514180: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-15 00:28:49.514209: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c433fbff4a2a): /proc/driver/nvidia/version does not exist\n",
      "2021-12-15 00:28:49.514686: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "!docker exec mf_gd python predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817579e9",
   "metadata": {},
   "source": [
    "# Score Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6135fe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: {'mse': 17.750801283164236, 'rmse': 4.2131699803312275, 'mae': 3.230549243527804, 'nmae': 4.309184466430007, 'smape': 103.95697621569137, 'r2': 0.3667788764852091}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 00:29:17.131255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 00:29:17.131323: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!docker exec mf_gd python score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae22ee9",
   "metadata": {},
   "source": [
    "# Check Outputs In mounted volume\n",
    "- data: this is where we mounted training and test data\n",
    "- logs: contains logged model training output\n",
    "- model: trained model artifacts\n",
    "- output: contains predictions. Also contains output from hyper-parameter tuning, if run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ab3bda16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml_vol:\n",
      "data\n",
      "logs\n",
      "model\n",
      "output\n",
      "\n",
      "ml_vol/data:\n",
      "test\n",
      "train\n",
      "\n",
      "ml_vol/data/test:\n",
      "ratings_test.csv\n",
      "\n",
      "ml_vol/data/train:\n",
      "attribute_defn.csv\n",
      "ratings_train.csv\n",
      "user_attributes.csv\n",
      "\n",
      "ml_vol/logs:\n",
      "\n",
      "ml_vol/model:\n",
      "checkpoint\n",
      "model_params\n",
      "model_weights.data-00000-of-00001\n",
      "model_weights.index\n",
      "preprocess_pipe.save\n",
      "\n",
      "ml_vol/output:\n",
      "predictions.csv\n",
      "scores.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -R ml_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c715c",
   "metadata": {},
   "source": [
    "# Push to docker hub \n",
    "Need permission to push to this repository on docker hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42b68c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/abudesai/rec_base_mf]\n",
      "5f70bf18a086: Preparing\n",
      "346efa7f5836: Preparing\n",
      "3c8526b9c9f7: Preparing\n",
      "6463cb47ff88: Preparing\n",
      "d82a4c4e92ff: Preparing\n",
      "155411760e3a: Preparing\n",
      "2cbb114c7605: Preparing\n",
      "459d9d53a256: Preparing\n",
      "831c5620387f: Preparing\n",
      "155411760e3a: Waiting\n",
      "459d9d53a256: Waiting\n",
      "2cbb114c7605: Waiting\n",
      "831c5620387f: Waiting\n",
      "5f70bf18a086: Layer already exists\n",
      "d82a4c4e92ff: Layer already exists\n",
      "6463cb47ff88: Layer already exists\n",
      "2cbb114c7605: Layer already exists\n",
      "459d9d53a256: Layer already exists\n",
      "155411760e3a: Layer already exists\n",
      "831c5620387f: Layer already exists\n",
      "346efa7f5836: Pushed\n",
      "3c8526b9c9f7: Pushed\n",
      "latest: digest: sha256:9c3fe7d38de8be4a52b6f06981cc3ca59ce057c04523c696f1d25f6b8a68d363 size: 2206\n"
     ]
    }
   ],
   "source": [
    "!docker push abudesai/rec_base_mf:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28d65813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/abudesai/rec_base_mf]\n",
      "5f70bf18a086: Preparing\n",
      "9e7efba54c94: Preparing\n",
      "ee11680e9331: Preparing\n",
      "6463cb47ff88: Preparing\n",
      "d82a4c4e92ff: Preparing\n",
      "155411760e3a: Preparing\n",
      "2cbb114c7605: Preparing\n",
      "459d9d53a256: Preparing\n",
      "831c5620387f: Preparing\n",
      "155411760e3a: Waiting\n",
      "2cbb114c7605: Waiting\n",
      "459d9d53a256: Waiting\n",
      "831c5620387f: Waiting\n",
      "d82a4c4e92ff: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "6463cb47ff88: Layer already exists\n",
      "ee11680e9331: Layer already exists\n",
      "155411760e3a: Layer already exists\n",
      "459d9d53a256: Layer already exists\n",
      "2cbb114c7605: Layer already exists\n",
      "831c5620387f: Layer already exists\n",
      "9e7efba54c94: Pushed\n",
      "latest: digest: sha256:2a683aff7de64bfc29eca126ca9e297b2a323ac8053cbe7d524a3161dc0ef8b2 size: 2206\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push abudesai/rec_base_mf:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521774e1",
   "metadata": {},
   "source": [
    "# Stop Container and Remove Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "659b7e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf_gd\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker stop mf_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e4b47a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf_gd\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rm mf_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae4a4504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untagged: abudesai/rec_base_mf:latest\n",
      "Untagged: abudesai/rec_base_mf@sha256:2a683aff7de64bfc29eca126ca9e297b2a323ac8053cbe7d524a3161dc0ef8b2\n",
      "Deleted: sha256:19d78cc6643cbc7bb56dbccc57c7b0c4945212be6b8277fc604cb95e71c352ec\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker rmi abudesai/rec_base_mf:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e0d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
